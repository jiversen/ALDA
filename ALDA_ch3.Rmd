---
title: "ALDA Examples"
output:
  html_document:
    code_folding: show
  pdf_document: default
---

<https://stats.idre.ucla.edu/r/examples/alda/>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = FALSE, eval=FALSE)

library(lattice)
library(latticeExtra)
library(grid)
library(plotrix)
library(tidyverse)

```

```{r function-defs, include=FALSE}
# inspired by: https://stackoverflow.com/questions/35627075/r-lattice-multiple-plot-page-how-to-put-text-in-page-margin
suptitle <- function(title, cex=1.5) {
  require(grid, quietly=T)
  vp<-viewport(x=0.5,y=1,width=1,height=0.05, just=c('center','top'))
  pushViewport(vp)
  grid.text(title,gp=gpar(cex=cex))
  popViewport()
}
```


# Chapter 3

The data are not publicly available, so it's not possible to recreate the chapter.
Instead, results taken from idre website, and code shown for illustration.

```{r load, eval=FALSE}
early.int <- read.table("d:/earlyint_pp.txt", header=T, sep=",")
early.int <- early.int %>% mutate(id = factor(id))
attach(early.int)
```

## Empirical Change Plots

```{r plot}
subset <- early.int[c(1:12, 175:186)]
subset

with(subset,
     xyplot( cog ~ age | id, data = subset, as.table=T,ylim=c(50,150),type=c("p","r"))
)

```

```{r}
   obs  id age cog program
1    1  68 1.0 103       1
2    2  68 1.5 119       1
3    3  68 2.0  96       1
4    4  70 1.0 106       1
5    5  70 1.5 107       1
6    6  70 2.0  96       1
7    7  71 1.0 112       1
8    8  71 1.5  86       1
9    9  71 2.0  73       1
10  10  72 1.0 100       1
11  11  72 1.5  93       1
12  12  72 2.0  87       1
13 175 902 1.0 119       0
14 176 902 1.5  93       0
15 177 902 2.0  99       0
16 178 904 1.0 112       0
17 179 904 1.5  98       0
18 180 904 2.0  79       0
19 181 906 1.0  89       0
20 182 906 1.5  66       0
21 183 906 2.0  81       0
22 184 908 1.0 117       0
23 185 908 1.5  90       0
24 186 908 2.0  76       0
```

![Figure 3.1](images/fig3_1-1.gif)

As in Chapter 2, you can examine the empirical distribution across individuals
of OLS intercept, slopes and $\sigma^2$. They note this inflates the apparent between-individual variability
since they are merely (fallible) estimates of 'true' change estimated from (noisy) observations. 
The size and distribution of $\sigma^2$ also gives some idea of how good the model is.

## Mixed effect model fitting

Statistical models describe hypothesized _population_ processes, not observed sample behavior.

### Formulation

#### Level 1

The outcome $Y_{ij}$ for each individual $i$ and measurement occasion $j$

$$
Y_{ij} = 
\underbrace{
\pi_{0i} + \pi_{1i} * TIME_{ij}
}_{structural~('true')}
+ 
\underbrace{
\epsilon_{ij}
}_{stochastic}
$$
A key concept is that the 'structural' part of the model represents an individual's *true* change trajectory, 
while the 'stochastic' part of the model, $\epsilon$, captures the *measurement error* in the actual observations _plus_
any part of the outcome not predicted by TIME (e.g. their $AGE$). 

Note that TIME is typically a 'centered' version of $AGE$ so that the intercept
can have a meaningful interpretation (e.g. using $TIME = AGE - AGE_{0}$ so that the intercept will be the 
true value of $Y$ at $AGE_{0}$, the 'initial status,' instead of the less-interpretable true value at $AGE = 0$).
In my analyses, typically also include baseline $AGE_0$ as well as $\Delta AGE$.

The $\pi$ are the *true* intercept and slope (the 'individual growth parameters') and 
$\epsilon_{ij}$ (the 'level-1 residuals') is the deviation of each observation from the 
true linear trend defined by subject-specific $\pi$ and is unmeasured and so
assumed to be *random error* and (for OLS) follow the distribution:


$$\epsilon_{ij} \sim N(0, \sigma_\epsilon^2)$$
This implies what they call 'classical' assumptions: IID and homoscedastic variance across timepoints and individuals.
However, they point out that in longitudinal data, any 'unexplained person-specific time-invariant' 
effect will create correlation of residuals across timepoints (within an individual). It's also possible that
the measurement precision will vary across timepoints, leading to residual variance being time-dependent
(aka heteroscedastic).

Promisingly, they say that the multi-level model 'accomodates automatically for certain kinds of complex error structure'
and that this will be discussed in Chapter 4 and 8.

Questions: 

* What if the $\epsilon$ are not distributed normally? Presumably that is evidence that the model
is under-specified, but what else can be learned? OTOH, one could make $\sigma_\epsilon^2$ zero by overfitting.
How is that balance struck? 
* What to do in the case that different individuals have different numbers of time points 
and different starting ages? Then $AGE_{0}$ would be different for each individual...


#### Level 2

This is a model for 'systematic inter-individual differences in change' that models differences
between individual change trajectories and (time-invariant) characteristics of the individual.
The outcomes are the individual growth parameters and are modeled with a true population intercept/slope, 
a true effect due to condition,
and a subject-specific stochastic deviation of each individual's growth parameters $\zeta$.

They put forward the image that each individual draws their growth parameters from an unknown 
random, bivariate population distribution.


$$\pi_{0i} = \gamma_{00} + \gamma_{01} * CONDITION_i + \zeta_{0i}
\\
\pi_{1i} = \gamma_{10} + \gamma_{11} * CONDITION_i + \zeta_{1i}$$

The $\zeta$ are jointly distributed as:

$$\begin{bmatrix}
\zeta_{0i} \\ \zeta_{1i}
\end{bmatrix}
\sim \mathcal{N}
\left(
\begin{bmatrix}
  0 \\ 0
\end{bmatrix},
\begin{bmatrix}
  \sigma_{0}^2 & \sigma_{01} \\ \sigma_{10} & \sigma_{1}^2
\end{bmatrix}
\right)$$



##### Fixed Effects ($\gamma$)
Describe systematic inter-individual differences in trajectories 
according to $CONDITION$ predictor variable(s)

Say $CONDITION$ is whether or not an individual received an intervention. 

- $\gamma_{•0}$ describe the average
hypothesized true trajectory of individuals not receiving the intervention. 
- $\gamma_{•1}$ capture the effect of 
the intervention, namely the hypothesized effect of the intervention on the growth model. 

*Question Answered*: "Does the intervention have an effect on individual growth?"
If these are non-zero, we can conclude the intervention had an effect on the growth.

##### Random Effects ($\zeta$):
Allows each individual's growth parameters $\pi$ to be scattered around the population means $\gamma$.

$\zeta$ is the portion of individual growth parameters $\pi$ not explained by level-2 predictors. 
It's descirbed by a bivariate normal distribution defined by a variance-covariance matrix.

$\sigma_{0~\lor~1}^2$ is the population variance in true individual parameters (intercept $\lor$ slope)
around the population averages $\gamma$. Called _conditional residual variances_ since describe portion of
variance left over after accounting for the model's predictor(s).

*Question Answered*: "How much heterogeneity in true change remains after accounting for the model's predictor(s)?"

$\sigma_{01}$ is the population covariance of individual intercept and slope.
Allowing for correlation of level-2 residuals allows for individual initial status (intercept) 
and rate of change (slope) to be correlated.

*Question Answered*: "Controlling for condition [e.g. intervention], 
are true initial status and true rate of change related?"

#### Graphical Depiction of the model
![Figure 3.4](images/fig3_4.png)

## Model Fit

```{r fit, eval=FALSE}
library(nlme)

model1<- lme(cog ~ time * program, data=early.int, random= ~time | id, method="ML")
summary(model1)
       
```

### Result

```{r}
       AIC      BIC    logLik 
  2385.942 2415.809 -1184.971

Random effects:
 Formula:  ~ time | id
 Structure: General positive-definite
               StdDev   Corr 
(Intercept) 11.135975 (Inter
       time  3.187612 -0.997
   Residual  8.644657       

Fixed effects: cog ~ time * program 
                 Value Std.Error  DF   t-value p-value 
 (Intercept)  107.8407  2.047915 204  52.65879  <.0001
        time  -21.1333  1.895694 204 -11.14807  <.0001
     program    6.8547  2.729082 101   2.51171  0.0136
time:program    5.2713  2.526229 204   2.08661  0.0382

Standardized Within-Group Residuals:
       Min         Q1        Med        Q3      Max 
 -2.347486 -0.5673839 0.02896275 0.5661698 2.330276

Number of Observations: 309
Number of Groups: 103
```

## Interpretation of Fixed Effects

### Fitted level-2 model
$$
\hat{\pi_{0i}} = \underbrace{107.84}_{Intercept} + \underbrace{6.85}_{program} * PROGRAM_i \\
\hat{\pi_{1i}} = \underbrace{-21.13}_{time} + \underbrace{5.27}_{time:program} * PROGRAM_i
$$
In words: True initial status of non-participant is 107.84. 
For the average participant, initial status is 6.85 higher (114.69). 
True annual rate of change for non-participant is -21.13. 
For the average participant, true rate of change is 5.27 higher (-15.86).

### Plotting Fits

Non-participant: $\hat{COG} = 107.84 -21.13 (AGE-1)$

Participant: $\hat{COG} = 114.69 -15.86 (AGE-1)$

```{r fits}
a <- fitted.values(model1)
interaction.plot(age, program, a, xlab="AGE", ylab="COG", 
                 ylim=c(50, 150), lwd=4, lty=1, col=4:5)

```

![Figure 3.5](images/figure3_5a.gif)
### Fixed effects parameter hypothesis tests

Null hypothesis: Controlling for all other predictors, the population value is 0.

$z = \frac{\hat(\gamma)}{ase(\hat(\gamma))}$ (3.7)

(Called _t-value_ in _lme_ output)

## Interpretation of Estimated Variance Components

Says it's trickier, as no absolute meaning or graphical aids...

The lme output has less info than in the book--it shows sigmas, but no standard errors or hypothesis,
tests so unclear how to eveluate. There must be a way to get at this more complete output.
ALSO, the actual values are different. Why?

Also, how test if assumptions are met?

$$\sigma_{\epsilon}^2 = 8.64^2 = 74.73  \\ \\
\begin{bmatrix}
  \sigma_{0}^2 & \sigma_{01} \\ \sigma_{10} & \sigma_{1}^2
\end{bmatrix}
= 
\begin{bmatrix}
  11.14^2 & 11.14\times3.19\times-0.997 \\ 11.14\times3.19\times-0.997 & 3.19^2
\end{bmatrix}
=
\begin{bmatrix}
  124.0 & -35.39 \\ -35.39 & 10.16
\end{bmatrix}$$


### Single-parameter variance components hypothesis tests

Is there any remaining residual outcome variation that could potentially be explained 
by other predictors? Yes, if $\sigma^2 \ne 0$ (i.e. reject null hypothesis).
_[This doesn't make sense--there should still be random measurement error, right?]_

Suggests where new predictors could be added.

*Utility questioned as sensitive to departures from normality, sample size, imbalance. Perhaps why
no such output is in the table...

In book, they find $\sigma_{\epsilon}^2$ and $\sigma_{0}^2$ are different from zero, suggesting
adding an additional Level-1 predictor (e.g. # books in home, or amount of parent interaction), 
as well as an additional Level-2 predictor(s), time invariant and time-varying, to explain the initial status.

The fact that neither of the slope variance/covariances ($\sigma_{1}^2$ and $\sigma_{01}$) are 
distinguished from 0 suggests that the intercepts and slopes of individual true change trajectories
are uncorrelated, and that random slope can be dropped.

## Model Fitting Methods

### Maximum Likelihood

ML Estimates are parameter values that maximize the probability of observing a particular sample of data.

_Likelihood Function_: $P(DATA | PARAMETERS)$. In practice, the _log-liklihood_ is maximized.

_Assumptions about residuals_: normal, mean 0, Level-1 independent of Level-2, 
all residuals independent of model's predictors.


## Pithy Statement
Outcome probability density's mean is determined by structural parts (Fixed Effects) 
and its variance is determined by stochastic parts (Random Effects)
